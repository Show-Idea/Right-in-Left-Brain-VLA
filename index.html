<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="Right-in-Left Brain VLA">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="Right-in-Left Brain VLA">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="KEYWORD1, KEYWORD2, KEYWORD3, machine learning, computer vision, AI">
  <!-- TODO: List all authors -->
  <meta name="author" content="FIRST_AUTHOR_NAME, SECOND_AUTHOR_NAME">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="INSTITUTION_OR_LAB_NAME">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="Right-in-Left Brain VLA">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="Right-in-Left Brain VLA">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="PAPER_TITLE - Research Preview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="FIRST_AUTHOR_NAME">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="KEYWORD1">
  <meta property="article:tag" content="KEYWORD2">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <!-- TODO: Replace with first author's Twitter handle -->
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="Right-in-Left Brain VLA">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="Right-in-Left Brain VLA">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="PAPER_TITLE - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="PAPER_TITLE">
  <meta name="citation_author" content="FIRST_AUTHOR_LAST, FIRST_AUTHOR_FIRST">
  <meta name="citation_author" content="SECOND_AUTHOR_LAST, SECOND_AUTHOR_FIRST">
  <meta name="citation_publication_date" content="2024">
  <meta name="citation_conference_title" content="CONFERENCE_NAME">
  <meta name="citation_pdf_url" content="https://YOUR_DOMAIN.com/static/pdfs/paper.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>Right-in-Left Brain VLA | Academic Research</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/png" href="static/images/logo.png">
  <link rel="apple-touch-icon" href="static/images/logo.png">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "Right-in-Left Brain VLA",
    "description": "Right-in-Left Brain VLA",
    "author": [
      {
        "@type": "Person",
        "name": "FIRST_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      },
      {
        "@type": "Person",
        "name": "SECOND_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      }
    ],
    "datePublished": "2024-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "CONFERENCE_OR_JOURNAL_NAME"
    },
    "url": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["KEYWORD1", "KEYWORD2", "KEYWORD3", "machine learning", "computer vision"],
    "abstract": "FULL_ABSTRACT_TEXT_HERE",
    "citation": "BIBTEX_CITATION_HERE",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "RESEARCH_AREA_1"
      },
      {
        "@type": "Thing", 
        "name": "RESEARCH_AREA_2"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "INSTITUTION_OR_LAB_NAME",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "logo": "https://YOUR_DOMAIN.com/static/images/image/png",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <!-- More Works Dropdown -->
  <div class="more-works-container">
    <button class="more-works-btn" onclick="toggleMoreWorks()" title="View More Works from Our Lab">
      <i class="fas fa-flask"></i>
      More Works
      <i class="fas fa-chevron-down dropdown-arrow"></i>
    </button>
    <div class="more-works-dropdown" id="moreWorksDropdown">
      <div class="dropdown-header">
        <h4>More Works from Our Lab</h4>
        <button class="close-btn" onclick="toggleMoreWorks()">
          <i class="fas fa-times"></i>
        </button>
      </div>
      <div class="works-list">
        <!-- TODO: Replace with your lab's related works -->
        <a href="https://arxiv.org/abs/PAPER_ID_1" class="work-item" target="_blank">
          <div class="work-info">
            <!-- TODO: Replace with actual paper title -->
            <h5>Paper Title 1</h5>
            <!-- TODO: Replace with brief description -->
            <p>Brief description of the work and its main contribution.</p>
            <!-- TODO: Replace with venue and year -->
            <span class="work-venue">Conference/Journal 2024</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <!-- TODO: Add more related works or remove extra items -->
        <a href="https://arxiv.org/abs/PAPER_ID_2" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Paper Title 2</h5>
            <p>Brief description of the work and its main contribution.</p>
            <span class="work-venue">Conference/Journal 2023</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="https://arxiv.org/abs/PAPER_ID_3" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Paper Title 3</h5>
            <p>Brief description of the work and its main contribution.</p>
            <span class="work-venue">Conference/Journal 2023</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
      </div>
    </div>
  </div>

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <div class="title-container">
              <img src="static/images/logo.png" alt="Logo" class="title-logo">
              <h1 class="title is-2 publication-title">
                Right-in-Left Brain VLA:<br> 
                <span style="white-space:nowrap;">A Closed-loop Perception-in-Reasoning</span><br>
                <span style="white-space:nowrap;">Vision-Language-Action Model for Robotic Manipulation</span>
              </h1>
            </div>
            <div class="is-size-4 publication-authors">
              <span class="author-block">anonymous authors</span>
            </div>
            <div class="publication-links">
              <span class="link-block">
                <a href="static/pdfs/paper.pdf" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fas fa-file-pdf"></i>
                </span>
                <span>Paper</span>
              </a>
            </span>

            <!-- TODO: Replace with your GitHub repository URL -->
            <span class="link-block">
              <a href="https://github.com/YOUR REPO HERE" target="_blank"
              class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <i class="fab fa-github"></i>
              </span>
              <span>Code (Coming soon)</span>
            </a>
          </span>  
                      
          <!-- TODO: Add your supplementary material or remove this section -->
          <span class="link-block">
            <a href="static/pdfs/constructed_labels_ycb_handal.zip" target="_blank"
            class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <i class="fas fa-database"></i>
            </span>
            <span>Constructed Labels on HANDAL & YCB (Coming soon)</span>
          </a>
        </span>
      </div>
    </div>
  </div>
</section>

<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body has-text-centered">
      <!-- 视频 -->
      <video 
        poster="" 
        id="tree" 
        autoplay 
        controls 
        muted 
        loop 
        preload="metadata"
        style="max-width: 90%; width: 800px; height: auto; border-radius: 12px; box-shadow: 0 4px 12px rgba(0,0,0,0.15);">
        <source src="static/videos/first_video.mp4" type="video/mp4">
      </video>

      <!-- 描述文字 -->
      <h2 class="subtitle has-text-centered" style="font-size:1rem; margin-top:1rem;">
        Pouring task visualizations of Right-in-Left Brain VLA, shown in overhead and palm views
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-2 has-text-black">Abstract</h2>
        <div class="content has-text-justified has-text-black">
          <!-- TODO: Replace with your paper abstract -->
          <p style="line-height: 1.4;">
            Robust robotic manipulation in open and dynamic environments is a fundamental requirement for making robots practical in human daily life. Without a closed loop that integrates perception, reasoning, and execution, robots cannot withstand disturbances and drift away from global goals, hindering practical deployment. Yet existing end-to-end Vision-Language-Action (VLA) models directly map inputs to actions but generalize poorly to unseen scenes in open environments. Hierarchical frameworks rely on lengthy prompt-driven spatial perception that is prone to hallucination, and their planning lacks closed-loop feedback, failing to correct online under disturbances. As a result, error accumulation destabilizes long-horizon task execution. Motivated by these limitations, we propose Right-in-Left Brain VLA, a brain-inspired framework that integrates perception into reasoning through a unified VLM. The “right brain” provides spatial perception by explicitly predicting affordances, poses, and constraints. The “left brain” performs chain-of-thought planning with continuous feedback from the “right brain” to ensure global consistency and correct errors online under disturbances. Experiments show that our method handles disturbances and mitigates error accumulation through closed-loop feedback and online adaptive correction to robustly execute long-horizon tasks and generalize to unseen scenarios.
          </p>
          <div class="has-text-centered" style="margin-top: 1.5rem;">
            <img src="static/images/abstract_figure.png" 
                 alt="Abstract Illustration"
                 style="max-width: 100%; height: auto; border-radius: 8px; box-shadow: 0 2px 6px rgba(0,0,0,0.15);display: block;margin-left: auto;margin-right: auto;"">
            <p style="font-size: 0.9rem; color: gray; margin-top: 0.5rem;">
              Comparison of Right-in-Left Brain VLA and previous methods
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Section 1 Template -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        
        <!-- 小标题 -->
        <h2 class="title is-4 has-text-black">Method-Affordance and manipulation constraint prediction</h2>
        
        <!-- 段落介绍 -->
        <div class="content has-text-justified has-text-black">
          <p style="line-height: 1.6;">
            We fine-tune the Qwen2-VL-7B to predict affordances and manipulation constraints, which mitigates hallucination and enables fast and reliable prediction.
          </p>
        </div>
        
        <!-- 图片 + caption -->
        <div class="has-text-centered" style="margin-top: 1.5rem;">
          <img src="static/images/Method1_figure.png" 
               alt="Method Illustration"
               style="max-width: 100%; height: auto; 
                      border-radius: 8px; 
                      box-shadow: 0 2px 6px rgba(0,0,0,0.15);
                      display: block; margin: 0 auto;">
          <p style="font-size: 0.9rem; color: gray; margin-top: 0.5rem;">
            Given multi-view images and task instructions, the model outputs structured tuples.
          </p>
        </div>
        <!-- 滚动图片展示 -->
        <section class="hero is-small" style="margin-top: 2rem;">
          <div class="hero-body">
            <div class="container">
              
              
              <!-- 统一介绍文字 -->
              <div class="content has-text-justified has-text-black" style="margin-bottom: 1.5rem;">
                <p style="line-height: 1.6;">
                  We construct unified supervision labels for affordance and manipulation constraint prediction based on the HANDAL and YCB datasets.
                </p>
              </div>
              <div id="results-carousel" class="carousel results-carousel">
                
                <div class="item">
                  <img src="static/images/carousel1.png" 
                       alt="Example result 1" loading="lazy" 
                       style="max-width: 120%; height: auto;" />
                  <h2 class="subtitle has-text-centered">
                    Ten representative household categories from HANDAL dataset.
                  </h2>
                </div>
                
                <div class="item">
                  <img src="static/images/carousel2.png" 
                       alt="Example result 2" loading="lazy"
                       style="max-width: 120%; height: auto;" />
                  <h2 class="subtitle has-text-centered">
                    Ten representative everday categories from YCB dataset.
                  </h2>
                </div>
              </div>
            </div>
          </div>
        </section>
        <!-- End carousel -->
      </div>
    </div>
  </div>
</section>

<!-- Section 2 Template -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        
        <!-- 小标题 -->
        <h2 class="title is-4 has-text-black">Method-Pose tracking and spatial constraints reasoning</h2>
        
        <!-- 段落介绍 -->
        <div class="content has-text-justified has-text-black">
          <p style="line-height: 1.6;">
            We integrate pose tracking with task-driven spatial reasoning, which provides closed-loop feedback and ensures feasible and plausible poses for objects interaction during execution.
          </p>
        </div>
        <!-- 视频集 -->
        <div class="hero is-small" style="margin-top: 2rem;">
          <div class="hero-body">
            <div class="container">
              <div id="results-carousel" class="carousel results-carousel">
                
                <div class="item">
                  <video controls muted loop preload="metadata" style="max-width:100%; border-radius:8px; box-shadow:0 2px 6px rgba(0,0,0,0.15);">
                    <source src="static/videos/carousel1_1.mp4" type="video/mp4">
                  </video>
                  <h2 class="subtitle has-text-centered">Pose tracking visualization of the power drill</h2>
                </div>
                
                <div class="item">
                  <video controls muted loop preload="metadata" style="max-width:100%; border-radius:8px; box-shadow:0 2px 6px rgba(0,0,0,0.15);">
                    <source src="static/videos/carousel1_2.mp4" type="video/mp4">
                  </video>
                  <h2 class="subtitle has-text-centered">Pose tracking visualization of a cube</h2>
                </div>
              </div>
            </div>
          </div>
        </div>
        <!-- End 视频集 -->
      </div>
    </div>
  </div>
</section>


<!-- Section 3 Template -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        
        <!-- 小标题 -->
        <h2 class="title is-4 has-text-black">Method-CoT Planning with Online Adaptive Correction</h2>
        
        <!-- 段落介绍 -->
        <div class="content has-text-justified has-text-black">
          <p style="line-height: 1.6;">
            We design a CoT planner with online adaptive correction to maintain global task consistency and robustness under disturbances.
          </p>
        </div>

        <!-- 图片 + caption -->
        <div class="has-text-centered" style="margin-top: 1.5rem;">
          <img src="static/images/Method3_figure.png" 
               alt="Method Illustration"
               style="max-width: 100%; height: auto; 
                      border-radius: 8px; 
                      box-shadow: 0 2px 6px rgba(0,0,0,0.15);
                      display: block; margin: 0 auto;">
          <p style="font-size: 0.9rem; color: gray; margin-top: 0.5rem;">
            Workflow of CoT planning with online adaptive correction. The planner first decomposes the task into an initial subtask pipeline, then alternates between subtask execution and feedback-driven replanning. Continuous perception from the “right brain” supports visual servoing for closed-loop execution, while feedback-based online replanning maintains global consistency under disturbances.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Section 4 Template -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        
        <!-- 小标题 -->
        <h2 class="title is-4 has-text-black">Method-Hybrid Execution with Analytic Control and RL</h2>
        
        <!-- 段落介绍 -->
        <div class="content has-text-justified has-text-black">
          <p style="line-height: 1.6;">
            We adopt a hybrid execution strategy with analytic control for simple tasks and reinforcement learning(RL) for complex and dexterous tasks. As analytic control is mature, we primarily detail the RL component, demonstrated on two representative tasks, namely drawer and cabinet opening.
          </p>
        </div>
        
        <!-- 视频集 -->
        <div class="hero is-small" style="margin-top: 2rem;">
          <div class="hero-body">
            <div class="container">
              <div id="results-carousel" class="carousel results-carousel">
                
                <div class="item">
                  <video controls muted loop preload="metadata" style="max-width:100%; border-radius:8px; box-shadow:0 2px 6px rgba(0,0,0,0.15);">
                    <source src="static/videos/carousel1_3.mp4" type="video/mp4">
                  </video>
                  <h2 class="subtitle has-text-centered">RL policies execution on drawer opening in simulation and the real world.</h2>
                </div>
                
                <div class="item">
                  <video controls muted loop preload="metadata" style="max-width:100%; border-radius:8px; box-shadow:0 2px 6px rgba(0,0,0,0.15);">
                    <source src="static/videos/carousel1_4.mp4" type="video/mp4">
                  </video>
                  <h2 class="subtitle has-text-centered">RL policies execution on cabinet opening in simulation and the real world</h2>
                </div>
              </div>
            </div>
          </div>
        </div>
        <!-- End 视频集 -->

      </div>
    </div>
  </div>
</section>
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4 has-text-black">Experimental Setup</h2>
        <div class="content has-text-justified has-text-black">
          <!-- TODO: Replace with your paper abstract -->
          <p style="line-height: 1.4;">
            We set up a real-world tabletop workspace with two JAKA ZU5 robots (6-DoF), equipped with a BrainCo Hand (11-DoF) and a DH AG95 parallel gripper (1-DoF).
          <div class="has-text-centered" style="margin-top: 1.5rem;">
            <img src="static/images/Experimental Setup_figure.png" 
                 alt="Abstract Illustration"
                 style="max-width: 100%; height: auto; border-radius: 8px; box-shadow: 0 2px 6px rgba(0,0,0,0.15);display: block;margin-left: auto;margin-right: auto;"">
            <p style="font-size: 0.9rem; color: gray; margin-top: 0.5rem;">
              Experimental Setup
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Experiment Visualization</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <!-- TODO: Add poster image for better preview -->
          <video poster="" id="video1" controls muted loop height="100%" preload="metadata">
            <!-- Your video file here -->
            <source src="static/videos/carousel1_5.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <!-- TODO: Add poster image for better preview -->
          <video poster="" id="video2" controls muted loop height="100%" preload="metadata">
            <!-- Your video file here -->
            <source src="static/videos/carousel1_6.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-10">
        <div class="content">

          <p style="font-size:14px; text-align:center;" >
            Thanks to <a href="https://nerfies.github.io" target="_blank">Nerfies</a> for the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Project Page Template</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
